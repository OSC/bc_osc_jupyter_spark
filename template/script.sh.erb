#!/bin/bash -l

<%- avail_cores = context.node_type.include?("hugemem") ? 48   : 28  -%>
<%- avail_mem   = context.node_type.include?("hugemem") ? 1528 : 120 -%>
<%- num_workers = context.num_workers.to_i -%>

<%-
  spark_config = {
    "spark.ui.reverseProxy" => "true",
    #"spark.ui.reverseProxyUrl" => "https://ondemand.osc.edu/node/${SPARK_MASTER_HOST}/${SPARK_MASTER_WEBUI_PORT}",
    "spark.authenticate" => "true",
    "spark.authenticate.secret" => "${SPARK_SECRET}",
    # Comment out below when reverse proxy and authentication are added
    # This still starts the Web UI on the master/worker procs, but disables it for
    # the Application driver
    "spark.ui.enabled" => "false",
    # So we need to disable the ability to kill applications
    "spark.ui.killEnabled" => "false",
  }
-%>

#
# Start Jupyter Notebook server + Spark cluster
#

# Load the required environment
setup_env () {
  module purge
  module load python/3.5 spark/2.1.0

  # Disable randomized hash for string in Python 3.3+
  export PYTHONHASHSEED=0
}
setup_env

#
# Launch Spark cluster in standalone mode
#

# Create log directory
export LOG_ROOT="${PWD}/logs"
mkdir "${LOG_ROOT}"

# Set master connection information
export SPARK_MASTER_HOST=${host}
export SPARK_MASTER_PORT=$(find_port ${SPARK_MASTER_HOST})
export SPARK_MASTER_WEBUI_PORT=$(find_port ${SPARK_MASTER_HOST})
export MASTER_LOG_FILE="${LOG_ROOT}/spark-master-${SPARK_MASTER_HOST}.out"

# Generate Spark secret
SPARK_SECRET="$(create_passwd)"

# Generate Spark configuration file
export SPARK_CONFIG_FILE="${PWD}/spark-defaults.conf"
(
umask 077
cat > "${SPARK_CONFIG_FILE}" << EOL
<%- spark_config.each do |k, v| -%>
<%= k %> <%= v %>
<%- end -%>
EOL
)

# Launch master
echo "Launching master on ${SPARK_MASTER_HOST}:${SPARK_MASTER_PORT}..."
set -x
"${SPARK_HOME}/bin"/spark-class "org.apache.spark.deploy.master.Master" \
    --properties-file "${SPARK_CONFIG_FILE}" \
  &> "${MASTER_LOG_FILE}" &
{ set +x; } 2>/dev/null

# Wait for the master server to fully start
echo "Waiting for master server to open port ${SPARK_MASTER_PORT}..."
if wait_until_port_used "${SPARK_MASTER_HOST}:${SPARK_MASTER_PORT}"; then
  echo "Discovered master listening on port ${SPARK_MASTER_PORT}!"
else
  echo "Timed out waiting for master to open port ${SPARK_MASTER_PORT}!" ; exit 1
fi
sleep 2

# Launch workers
echo "Launching workers..."
for ((i=0; i<<%= num_workers %>; i++)); do
  pbsdsh -u bash -l -c "
    <%- if context.only_driver_on_root == "1" -%>
    [[ \${PBS_NODENUM} == "0" ]] && exit 0
    <%- end -%>
    $(declare -f source_helpers)
    source_helpers

    # Load the required environment
    $(declare -f setup_env)
    setup_env

    # Set worker connection information
    export SPARK_WORKER_HOST=\${HOSTNAME}
    export SPARK_WORKER_PORT=\$(find_port \${SPARK_WORKER_HOST})
    export SPARK_WORKER_WEBUI_PORT=\$(find_port \${SPARK_WORKER_HOST})
    export SPARK_WORKER_DIR=\"${PWD}/work\"
    export SPARK_WORKER_CORES=<%= avail_cores / num_workers %>
    export WORKER_LOG_FILE=\"${LOG_ROOT}/spark-worker-\${SPARK_WORKER_HOST}-${i}.out\"

    # Launch worker
    echo \"Launching worker #${i} on \${SPARK_WORKER_HOST}:\${SPARK_WORKER_PORT}...\"
    set -x
    \"${SPARK_HOME}/bin\"/spark-class \"org.apache.spark.deploy.worker.Worker\" \
        --properties-file \"${SPARK_CONFIG_FILE}\" \
        spark://${SPARK_MASTER_HOST}:${SPARK_MASTER_PORT} \
      &> \"\${WORKER_LOG_FILE}\"
  " &
done

#
# Launch Jupyter with PySpark
#

# Use Jupyter instead of iPython
export PYSPARK_DRIVER_PYTHON="jupyter"

# Command line arguments for Jupyter
export PYSPARK_DRIVER_PYTHON_OPTS="notebook --config='${CONFIG_FILE}'"

# Set working directory to home directory
cd "${HOME}"

# Launch Jupyter Notebook interface to Spark
echo "Launching Jupyter..."
set -x
pyspark \
  --master spark://${SPARK_MASTER_HOST}:${SPARK_MASTER_PORT} \
  --driver-memory <%= context.only_driver_on_root == "1" ? avail_mem : 2 %>G \
  --executor-memory <%= avail_mem / num_workers %>G \
  --conf spark.driver.maxResultSize=0 \
  --properties-file "${SPARK_CONFIG_FILE}"
