#!/usr/bin/env bash

<%-
  avail_cores = context.node_type.include?("hugemem") ? 48   : 28
  avail_mem   = context.node_type.include?("hugemem") ? 1528 : 120
  num_workers = context.num_workers.to_i

  spark_config = {
    "spark.ui.reverseProxy" => "true",
    #"spark.ui.reverseProxyUrl" => "https://ondemand.osc.edu/node/${SPARK_MASTER_HOST}/${SPARK_MASTER_WEBUI_PORT}",
    "spark.authenticate" => "true",
    "spark.authenticate.secret" => "${SPARK_SECRET}",
    # Comment out below when reverse proxy and authentication are added
    # This still starts the Web UI on the master/worker procs, but disables it for
    # the Application driver
    "spark.ui.enabled" => "false",
    # So we need to disable the ability to kill applications
    "spark.ui.killEnabled" => "false",
  }
-%>

#
# Start Jupyter Notebook server + Spark cluster
#

# Load the required environment
setup_env () {
  module purge
}
setup_env

# Load the runtime environment
runtime_env() {
  module purge
  module load python/3.6 spark/2.3.0

  # Disable randomized hash for string in Python 3.3+
  export PYTHONHASHSEED=0
}

#
# Launch Spark cluster in standalone mode
#

# Create log directory
export LOG_ROOT="${PWD}/logs"
mkdir "${LOG_ROOT}"

# Set master connection information
export SPARK_MASTER_HOST=${host}
export SPARK_MASTER_PORT=$(find_port ${SPARK_MASTER_HOST})
export SPARK_MASTER_WEBUI_PORT=$(find_port ${SPARK_MASTER_HOST})

# Generate Spark secret
SPARK_SECRET="$(create_passwd)"

# Generate Spark configuration file
export SPARK_CONFIG_FILE="${PWD}/spark-defaults.conf"
(
umask 077
cat > "${SPARK_CONFIG_FILE}" << EOL
<%- spark_config.each do |k, v| -%>
<%= k %> <%= v %>
<%- end -%>
EOL
)

# Launch Spark master process
(
# Load the required environment
runtime_env

# Set master information
export MASTER_LOG_FILE="${LOG_ROOT}/spark-master-${SPARK_MASTER_HOST}.out"

# Launch master
echo "Launching master on ${SPARK_MASTER_HOST}:${SPARK_MASTER_PORT}..."
set -x
"${SPARK_HOME}/bin"/spark-class "org.apache.spark.deploy.master.Master" \
    --properties-file "${SPARK_CONFIG_FILE}" \
  &> "${MASTER_LOG_FILE}" &
)

# Wait for the master server to fully start
echo "Waiting for master server to open port ${SPARK_MASTER_PORT}..."
if wait_until_port_used "${SPARK_MASTER_HOST}:${SPARK_MASTER_PORT}" 60; then
  echo "Discovered master listening on port ${SPARK_MASTER_PORT}!"
else
  echo "Timed out waiting for master to open port ${SPARK_MASTER_PORT}!"
  clean_up 1
fi
sleep 2

# Create Spark worker launcher script
export SPARK_WORKER_SCRIPT="${PWD}/spark-worker.sh"
(
umask 077
sed 's/^ \{2\}//' > "${SPARK_WORKER_SCRIPT}" << EOL
  #!/usr/bin/bash -l

  <%- if context.only_driver_on_root == "1" -%>
  [[ \${PBS_NODENUM} == "0" ]] && exit 0
  <%- end -%>

  # Load helper methods
  $(declare -f source_helpers)
  source_helpers

  # Load the required environment
  $(declare -f runtime_env)
  runtime_env

  # Set worker connection information
  export SPARK_WORKER_HOST=\${HOSTNAME}
  export SPARK_WORKER_PORT=\$(find_port \${SPARK_WORKER_HOST})
  export SPARK_WORKER_WEBUI_PORT=\$(find_port \${SPARK_WORKER_HOST})
  export SPARK_WORKER_DIR="${PWD}/work"
  export SPARK_WORKER_CORES=<%= avail_cores / num_workers %>
  export WORKER_LOG_FILE="${LOG_ROOT}/spark-worker-\${SPARK_WORKER_HOST}-\${1}.out"

  # Launch worker
  echo "Launching worker #\${1} on \${SPARK_WORKER_HOST}:\${SPARK_WORKER_PORT}..."
  set -x
  "\${SPARK_HOME}/bin"/spark-class "org.apache.spark.deploy.worker.Worker" \\
      --properties-file "${SPARK_CONFIG_FILE}" \\
      spark://${SPARK_MASTER_HOST}:${SPARK_MASTER_PORT} \\
    &> "\${WORKER_LOG_FILE}"
EOL
)
chmod 700 "${SPARK_WORKER_SCRIPT}"

# Launch workers
echo "Launching workers..."
for ((i=0; i<<%= num_workers %>; i++)); do
  pbsdsh -u -- "${SPARK_WORKER_SCRIPT}" ${i} &
done

#
# Launch Jupyter with PySpark
#

# Create a `python` wrapper script
export PYTHON_WRAPPER_FILE="${PWD}/python.sh"
(
umask 077
sed 's/^ \{2\}//' > "${PYTHON_WRAPPER_FILE}" << EOL
  #!/usr/bin/env bash

  # Log all output from this script
  export PYTHON_LOG_FILE="${PWD}/python.log"
  exec &>>"\${PYTHON_LOG_FILE}"

  # Load the required environment
  $(declare -f runtime_env)
  runtime_env

  # Setup PySpark
  export PYTHONPATH="\${SPARK_HOME}/python:\$PYTHONPATH"
  export PYTHONPATH="\${SPARK_HOME}/python/lib/py4j-0.10.6-src.zip:\$PYTHONPATH"
  export PYTHONSTARTUP="\${SPARK_HOME}/python/pyspark/shell.py"
  export PYSPARK_SUBMIT_ARGS=" \\
    --master spark://${SPARK_MASTER_HOST}:${SPARK_MASTER_PORT} \\
    --driver-memory <%= context.only_driver_on_root == "1" ? avail_mem : 2 %>G \\
    --executor-memory <%= avail_mem / num_workers %>G \\
    --conf spark.driver.maxResultSize=0 \\
    --properties-file \"${SPARK_CONFIG_FILE}\" \\
    pyspark-shell \\
  "

  # Launch the original command
  set -x
  exec python "\${@}"
EOL
)
chmod 700 "${PYTHON_WRAPPER_FILE}"

# Create Jupyter kernels
export JUPYTER_PATH="${PWD}/jupyter"
export KERNEL_PATH="${JUPYTER_PATH}/kernels/pyspark"
mkdir -p "${KERNEL_PATH}"
cp "${PWD}/assets/python"/* "${KERNEL_PATH}"
(
umask 077
sed 's/^ \{2\}//' > "${KERNEL_PATH}/kernel.json" << EOL
  {
    "display_name": "PySpark",
    "language": "python",
    "argv": [
      "${PYTHON_WRAPPER_FILE}",
      "-m",
      "ipykernel",
      "-f",
      "{connection_file}"
    ]
  }
EOL
)

# Output debug information
set -x
module list
{ set +x; } 2>/dev/null

# Set working directory to notebook root directory
cd "${NOTEBOOK_ROOT}"

# Setup Jupyter Notebook environment
export PATH="/usr/local/anaconda-notebook/bin:${PATH}"

# List available kernels for debugging purposes
set -x
jupyter kernelspec list
{ set +x; } 2>/dev/null

# Launch Jupyter Notebook interface to Spark
set -x
jupyter notebook --config="${CONFIG_FILE}"
