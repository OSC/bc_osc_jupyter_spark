#!/bin/bash -l

<%- avail_cores = context.node_type.include?("hugemem") ? 48   : 28  -%>
<%- avail_mem   = context.node_type.include?("hugemem") ? 1528 : 120 -%>
<%- num_workers = context.num_workers.to_i -%>

#
# Start Jupyter Notebook Server + Spark cluster
#

# Restore the module environment to avoid conflicts
module restore

# Load the required modules
module load python/3.5 spark/2.1.0

#
# Launch Spark cluster in standalone mode
#

# Create log directory
export LOG_ROOT="${PWD}/logs"
mkdir "${LOG_ROOT}"

# Set master connection information
export SPARK_MASTER_HOST=${host}
export SPARK_MASTER_PORT=$(find_port ${SPARK_MASTER_HOST})
export SPARK_MASTER_WEBUI_PORT=$(find_port ${SPARK_MASTER_HOST})
export MASTER_LOG_FILE="${LOG_ROOT}/spark-master-${SPARK_MASTER_HOST}.out"

# Generate Spark configuration file
export SPARK_CONFIG_FILE="${PWD}/spark-defaults.conf"
(
umask 077
cat > "${SPARK_CONFIG_FILE}" << EOL
spark.ui.reverseProxy     true
#spark.ui.reverseProxyUrl  https://ondemand.osc.edu/node/${SPARK_MASTER_HOST}/${SPARK_MASTER_WEBUI_PORT}
spark.authenticate        true
spark.authenticate.secret $(create_passwd)
# Comment out below when reverse proxy with path is resolved
spark.ui.enabled          false
EOL
)

# Launch master
echo "Launching master on ${SPARK_MASTER_HOST}:${SPARK_MASTER_PORT}..."
"${SPARK_HOME}/bin"/spark-class "org.apache.spark.deploy.master.Master" \
    --properties-file "${SPARK_CONFIG_FILE}" \
  &> "${MASTER_LOG_FILE}" &

# Wait for the master server to fully start
echo "Waiting for master server to open port ${SPARK_MASTER_PORT}..."
if wait_until_port_used "${SPARK_MASTER_HOST}:${SPARK_MASTER_PORT}"; then
  echo "Discovered master listening on port ${SPARK_MASTER_PORT}!"
else
  echo "Timed out waiting for master to open port ${SPARK_MASTER_PORT}!" ; exit 1
fi
sleep 2

# Launch workers
echo "Launching workers..."
for ((i=0; i<<%= num_workers %>; i++)); do
  pbsdsh -u bash -c "
    <%- if context.only_driver_on_root == "1" -%>
    [[ \${PBS_NODENUM} == "0" ]] && exit 0
    <%- end -%>
    $(declare -f source_helpers)
    source_helpers

    # Set worker connection information
    export SPARK_WORKER_HOST=\${HOSTNAME}
    export SPARK_WORKER_PORT=\$(find_port \${SPARK_WORKER_HOST})
    export SPARK_WORKER_WEBUI_PORT=\$(find_port \${SPARK_WORKER_HOST})
    export SPARK_WORKER_DIR=\"${PWD}/work\"
    export SPARK_WORKER_CORES=<%= avail_cores / num_workers %>
    export WORKER_LOG_FILE=\"${LOG_ROOT}/spark-worker-\${SPARK_WORKER_HOST}-${i}.out\"

    # Launch worker
    echo \"Launching worker #${i} on \${SPARK_WORKER_HOST}:\${SPARK_WORKER_PORT}...\"
    \"${SPARK_HOME}/bin\"/spark-class \"org.apache.spark.deploy.worker.Worker\" \
        --properties-file \"${SPARK_CONFIG_FILE}\" \
        spark://${SPARK_MASTER_HOST}:${SPARK_MASTER_PORT} \
      &> \"\${WORKER_LOG_FILE}\"
  " &
done

#
# Launch Jupyter with PySpark
#

# Fixes issue where it launches system-installed python instead of one in PATH
export PYSPARK_PYTHON="$(command -v python)"

# Use Jupyter instead of iPython
export PYSPARK_DRIVER_PYTHON="jupyter"

# Command line arguments for Jupyter
export PYSPARK_DRIVER_PYTHON_OPTS="notebook --config='${CONFIG_FILE}'"

# Set working directory to home directory
cd "${HOME}"

# Launch Jupyter Notebook interface to Spark
echo "Launching Jupyter..."
pyspark \
  --master spark://${SPARK_MASTER_HOST}:${SPARK_MASTER_PORT} \
  --driver-memory <%= context.only_driver_on_root == "1" ? avail_mem : 2 %>G \
  --executor-memory <%= avail_mem / num_workers %>G \
  --conf spark.driver.maxResultSize=0 \
  --properties-file "${SPARK_CONFIG_FILE}"
